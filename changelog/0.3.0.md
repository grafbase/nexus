# Nexus 0.3.0 - August 15, 2025

## Summary

This release introduces token-based rate limiting capabilities for LLM providers, enabling fine-grained control over AI resource consumption with per-user and group-based quotas. Additionally, explicit model configuration is now enforced for all LLM providers, improving security and preventing unintended model usage. The release includes comprehensive infrastructure for these new features and various dependency updates to maintain security and performance.

## New Features

### Token-Based Rate Limiting for LLMs
**User Impact:** Administrators can now enforce token consumption limits on LLM usage, preventing abuse and controlling costs through per-user and group-based rate limiting.

**Technical Details:**
- Hierarchical rate limiting system with provider-level and model-level configurations
- Support for user groups with different token quotas (e.g., premium vs basic users)
- Input token counting with accurate calculation for each provider (OpenAI, Anthropic, Google)
- Redis-backed distributed rate limiting for scalability across multiple instances
- In-memory rate limiting with mini-moka cache for single-instance deployments
- Client identification middleware for extracting user/group information from JWT tokens
- Files added: `crates/server/src/client_id.rs`, `crates/rate-limit/src/token.rs`, `crates/llm/src/token_counter.rs`
- Configuration: New `rate_limits.per_user` sections at provider and model levels
- Rate limit hierarchy: Model+Group > Model Default > Provider+Group > Provider Default

### Explicit Model Configuration Requirement
**User Impact:** LLM providers now require explicit model configuration, preventing accidental access to unintended or expensive models. This ensures only approved models are accessible through the router.

**Technical Details:**
- Mandatory model configuration for all LLM providers
- Model aliasing support for creating user-friendly names
- Centralized ModelManager for consistent model resolution
- Validation at startup to ensure at least one model is configured
- Files modified: `crates/llm/src/provider/model_manager.rs`, `crates/config/src/llm.rs`
- Configuration: Models must be explicitly listed under `[llm.providers.<name>.models.<model-id>]`

### Stateless Client Identification System
**User Impact:** Enhanced security and tracking through a completely stateless user identification mechanism that automatically extracts identity from authentication tokens or headers, enabling personalized rate limiting and usage tracking without maintaining server-side session state.

**Technical Details:**
- **Stateless Architecture:** No server-side session storage required - all user context derived from request headers
- **Multiple Identification Sources:**
  - JWT tokens: Automatic extraction from Authorization header (Bearer tokens)
  - Custom headers: Support for `X-Client-Id` and other configurable headers
  - OAuth2 tokens: Integration with external OAuth2 providers for token validation
- **Claim Extraction:**
  - Standard JWT claims: `sub` (subject) and `email` for user identification
  - Custom group claims: Configurable claim names for group membership extraction
  - Fallback hierarchy: Multiple claim sources with configurable precedence
- **Zero Session State:** Each request is independently authenticated and authorized
- **Group Validation:** Stateless validation against pre-configured allowed groups
- **Performance:** Minimal overhead through claim caching within request lifecycle
- Files added: `crates/server/src/client_id/middleware.rs`, `crates/server/src/auth/claims.rs`
- Configuration: New `[server.client_identification]` section with validation options
- **Stateless Benefits:**
  - Horizontal scaling without session synchronization
  - No session storage requirements (no Redis/database for sessions)
  - Resilient to server restarts - no session loss
  - Perfect for containerized and serverless deployments

## Enhancements

### Improved Rate Limiting Architecture
**User Impact:** More reliable and scalable rate limiting with support for both in-memory and Redis-backed storage, suitable for single-instance and distributed deployments.

**Technical Details:**
- Averaging fixed window algorithm for Redis-based rate limiting
- Token bucket algorithm with Governor for in-memory rate limiting
- Lua scripts for atomic Redis operations
- Comprehensive edge case handling (negative tokens, overflow protection)
- Files modified: `crates/rate-limit/src/storage/memory.rs`, `crates/rate-limit/src/storage/redis.rs`
- Added: `crates/rate-limit/src/storage/redis/rate_limit_tokens.lua`

### Enhanced Configuration Validation
**User Impact:** Clearer error messages and validation at startup, preventing misconfiguration and ensuring all required settings are properly defined.

**Technical Details:**
- Validation of group values referenced in rate limit configurations
- Enforcement of model configuration requirements
- Improved error messages for missing or invalid configurations
- Files modified: `crates/config/src/loader.rs`, `crates/config/src/lib.rs`

### Comprehensive Test Coverage
**User Impact:** Increased reliability through extensive testing of new features, ensuring robust behavior under various scenarios.

**Technical Details:**
- Added 1000+ lines of integration tests for token limiting
- Edge case testing for rate limit overflow and boundary conditions
- Group fallback behavior testing
- Redis-based distributed rate limiting tests
- OAuth2 integration tests for client identification
- Files added: `crates/integration-tests/tests/llm/token_limiting.rs` and subdirectories

## Bug Fixes

### Fixed Configuration Validation for Enabled Services
**Issue:** Configuration validation was incorrectly requiring servers when features were enabled, causing tests to fail.
**Resolution:** Separated enabled flags from server presence checks, allowing features to be enabled without downstream servers.
**Technical:** Updated McpConfig and LlmConfig with separate `enabled()` and `has_servers()`/`has_providers()` methods.

## Infrastructure & Dependencies

### Dependency Updates
- **async-trait**: Updated to 0.1.89
- **reqwest**: Updated to 0.12.23
- **clap**: Updated to 4.5.45
- **thiserror**: Updated to 2.0.14
- **anyhow**: Updated to 1.0.99
- **libc**: Updated to 0.2.175
- **uuid**: Updated to 1.18.0

### CI/CD Improvements
- Updated `taiki-e/install-action` to v2.58.12
- Updated `depot/build-push-action` to latest digest

## Deployment Notes

### Breaking Changes
- **Model Configuration Required**: All LLM providers now require explicit model configuration. Deployments must update their configuration files to include model definitions.
- **Rate Limit Configuration**: If using token-based rate limiting, client identification must be enabled and properly configured.

### Migration Steps
1. **Update LLM Configuration**: Add explicit model configurations for all providers:
   ```toml
   [llm.providers.openai.models.gpt-4]
   # Optional: rename = "different-model-id"

   [llm.providers.openai.models."gpt-3.5-turbo"]
   ```

2. **Configure Token Rate Limits** (if needed):
   ```toml
   [server.client_identification]
   enabled = true

   [server.client_identification.validation]
   group_values = ["premium", "basic"]

   [llm.providers.openai.rate_limits.per_user]
   input_token_limit = 1000
   interval = "60s"

   [llm.providers.openai.rate_limits.per_user.groups.premium]
   input_token_limit = 5000
   interval = "60s"
   ```

3. **Update JWT Claims**: Ensure your authentication system includes appropriate claims for user identification (`sub` or `email`) and group membership if using group-based rate limits.

### Configuration Example
```toml
[server]
host = "0.0.0.0"
port = 3000

[server.client_identification]
enabled = true

[server.client_identification.validation]
group_values = ["premium", "basic", "enterprise"]

[llm]
enabled = true

[llm.providers.openai]
type = "openai"
api_key = "sk-..."  # Or use BYOK

# Explicit model configuration (required)
[llm.providers.openai.models."gpt-4o"]
rename = "gpt-4o-0123"  # The real name of the model in OpenAI

[llm.providers.openai.models."gpt-4o-mini"]

# Provider-level rate limits
[llm.providers.openai.rate_limits.per_user]
input_token_limit = 1000
interval = "60s"

[llm.providers.openai.rate_limits.per_user.groups.premium]
input_token_limit = 10000
interval = "60s"

# Model-specific rate limits (override provider defaults)
[llm.providers.openai.models."gpt-4o".rate_limits.per_user]
input_token_limit = 500
interval = "60s"
```

### Performance Considerations
- Token counting adds minimal overhead (< 1ms per request)
- Redis-based rate limiting recommended for distributed deployments
- In-memory rate limiting suitable for single-instance deployments
- Group validation is cached to minimize JWT parsing overhead
