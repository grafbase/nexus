# Nexus 0.5.0 - September 18, 2025

## Summary

This release introduces native Anthropic API support, multi-endpoint LLM configuration, and security through the adoption of [Chainguard's Wolfi](https://www.chainguard.dev/unchained/introducing-wolfi-the-first-linux-un-distro-designed-for-securing-the-software-supply-chain) base image for Docker deployments.

## New Features

### Native Anthropic API Support

**User Impact:** Full native support for the Anthropic API protocol. Users can now interact with Claude using Anthropic's native message format, tool calling conventions, and streaming patterns. The implementation is tested with Claude Code, which works fluently with many of the models supported by Nexus, excluding the ones with no tool calling support.

**Technical Details:**
- **Protocol Support:** Complete Anthropic Messages API implementation
- **Files Added:**
  - `crates/llm/src/messages/anthropic.rs` - Anthropic message types
  - `crates/integration-tests/tests/llm/anthropic/` - Test suite
- **Features:**
  - Native tool calling with Anthropic's specific format
  - Streaming support with proper event handling
  - Claude Code compatibility

### Enhanced Security with Chainguard Wolfi Base Image

**User Impact:** Improved security for Docker deployments. The new Chainguard Wolfi-based container image reduces attack surface and eliminates known vulnerabilities present in the previous Debian-based image.

**Security Improvements:**
- **Previous Debian Image:** 48 known vulnerabilities (2 Critical, 16 High, 25 Medium, 5 Low)
- **New Wolfi Image:** Minimal attack surface with distroless design
- **Benefits:**
  - Reduced container size (smaller attack surface)
  - Regular security updates from Chainguard
  - Hardened by default with minimal packages
  - Compliance-ready for security-conscious deployments

**Technical Details:**
- **Base Image:** `cgr.dev/chainguard/wolfi-base:latest`
- **Migration:** Seamless upgrade - no configuration changes required
- **Compatibility:** Full backward compatibility maintained
- **File Modified:** `Dockerfile` - Complete rebuild with multi-stage optimization

### Multi-Endpoint LLM Configuration

**User Impact:** Configure multiple LLM endpoints with different protocols on the same Nexus instance. This allows serving both OpenAI and Anthropic compatible endpoints simultaneously, making it easier to support diverse client applications.

**Technical Details:**
- **Configuration:** New `[llm.protocols]` section in configuration
- **Files Modified:**
  - `crates/config/src/llm.rs` - Enhanced endpoint configuration parsing
  - `crates/server/src/lib.rs` - Multi-endpoint routing setup
- **Example Configuration:**
  ```toml
  [llm.protocols.openai]
  enabled = true
  path = "/llm/openai"

  [llm.protocols.anthropic]
  enabled = true
  path = "/llm/anthropic"
  ```
- **Routing:** Dynamic endpoint registration based on configuration
- **Default Behavior:** OpenAI endpoint enabled by default at `/llm/openai`, Anthropic disabled by default

### Dependency Updates

**User Impact:** Improved stability, performance, and security through regular dependency maintenance.

**Key Updates:**
- **Rust Crates:**
  - `rmcp` to 0.6.4 - MCP protocol improvements and bug fixes
  - `aws-sdk-bedrockruntime` to 1.106.0 - Latest AWS Bedrock support
  - `tonic` to 0.14.2 - gRPC/OpenTelemetry communication improvements
  - Various internal dependencies updated for stability
- **CI/CD:**
  - GitHub Actions tooling kept up-to-date
- **Security:** Regular updates to maintain security posture, no critical CVEs addressed

## Breaking Changes

### LLM Endpoint Configuration

The LLM endpoint configuration has changed from a single path to protocol-specific paths:

**Previous (0.4.x):**
```toml
[llm]
enabled = true
path = "/llm"  # Single endpoint path
```

**New (0.5.0):**
```toml
[llm]
enabled = true

[llm.protocols.openai]
enabled = true
path = "/llm"  # OpenAI protocol endpoint

[llm.protocols.anthropic]
enabled = false  # Disabled by default
path = "/llm/anthropic"
```

**Migration Guide:**
- The `llm.path` field has been removed
- Replace with `llm.protocols.openai.path` for OpenAI-compatible endpoints
- Default path changed from `/llm` to `/llm/openai` if not explicitly configured
- To maintain the same endpoint path, set `llm.protocols.openai.path = "/llm"`

## Deployment Notes

### Configuration Updates

**Minimal migration from 0.4.x:**
```toml
# Old configuration (0.4.x):
# [llm]
# path = "/llm"

# New configuration (0.5.0):
[llm]
enabled = true

[llm.protocols.openai]
enabled = true
path = "/llm"  # Keep the same path as before
```

### Migration Considerations

1. **Required Configuration Update:** Update LLM endpoint configuration from `llm.path` to `llm.protocols.openai.path` (see Breaking Changes section)
2. **Docker Image:** The new Chainguard Wolfi-based image is a drop-in replacement with significantly improved security
3. **Optional Enhancements:** Consider adding Anthropic endpoints if using Claude models
4. **Testing Recommendation:** Run integration tests to verify provider compatibility
