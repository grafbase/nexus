use std::borrow::Cow;

use serde::{Deserialize, Serialize};

use crate::messages::{
    ChatChoice, ChatChoiceDelta, ChatCompletionChunk, ChatCompletionResponse, ChatMessage, ChatMessageDelta, ChatRole,
    Model, ObjectType, Usage,
};

/// Reason why the model stopped generating tokens.
///
/// Indicates the completion status of the generation.
#[derive(Debug, Deserialize, PartialEq)]
pub(super) enum FinishReason {
    /// Natural stop point was reached.
    #[serde(rename = "STOP")]
    Stop,
    /// Maximum number of tokens was reached.
    #[serde(rename = "MAX_TOKENS")]
    MaxTokens,
    /// Content was filtered due to safety concerns.
    #[serde(rename = "SAFETY")]
    Safety,
    /// Content was filtered due to recitation concerns (potential plagiarism).
    #[serde(rename = "RECITATION")]
    Recitation,
    /// Explicitly marked as "OTHER" by Google.
    #[serde(rename = "OTHER")]
    Other,
    /// Any other finish reason not yet known.
    /// Captures the actual string value for forward compatibility.
    #[serde(untagged)]
    Unknown(String),
}

/// Represents content in a conversation with Gemini.
///
/// Used for both input (user/system messages) and output (model responses).
/// This type is shared between input and output as Google uses the same structure for both.
#[derive(Debug, Deserialize, Serialize)]
pub(super) struct GoogleContent {
    /// The parts that compose this content.
    ///
    /// A content can have multiple parts, each containing different types of data
    /// (text, images, etc.), though currently we only support text.
    pub(super) parts: Vec<GooglePart>,

    /// The role of the content creator.
    ///
    /// Valid values are:
    /// - "user": Input provided by the user
    /// - "model": Response from the model
    /// - For system instructions, use "user" role
    pub(super) role: String,
}

/// A single part of content.
///
/// Represents an atomic piece of content, such as text or an image.
/// Currently only text is supported in this implementation.
#[derive(Debug, Deserialize, Serialize)]
pub(super) struct GooglePart {
    /// Text content of this part.
    ///
    /// Will be `None` for non-text content types (e.g., images, which are not yet supported).
    pub(super) text: Option<String>,
}

/// Response from Google Gemini GenerateContent API.
///
/// Contains the model's generated response along with metadata.
/// Documented in the [Google AI API Reference](https://ai.google.dev/api/generate-content#response-body).
#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub(super) struct GoogleGenerateResponse {
    /// List of response candidates from the model.
    ///
    /// Usually contains a single candidate unless `candidate_count` was set in the request.
    pub(super) candidates: Vec<GoogleCandidate>,

    /// Token usage metadata for this generation.
    ///
    /// Includes counts for input tokens, output tokens, and total.
    /// May be absent in some API responses.
    pub(super) usage_metadata: Option<GoogleUsageMetadata>,
}

/// A response candidate generated by the model.
///
/// Represents one possible response to the input prompt.
#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub(super) struct GoogleCandidate {
    /// Generated content from the model.
    pub(super) content: GoogleContent,

    /// The reason why the model stopped generating tokens.
    /// May be absent in streaming or partial responses.
    pub(super) finish_reason: Option<FinishReason>,

    /// Index of this candidate in the list of candidates.
    /// Defaults to 0 if not provided.
    #[serde(default)]
    pub(super) index: i32,

    /// Safety ratings for the generated content.
    ///
    /// Indicates probability of harmfulness across various categories.
    #[allow(dead_code)]
    pub(super) safety_ratings: Option<Vec<GoogleSafetyRating>>,
}

/// Safety rating for generated content.
///
/// Indicates the probability that content is harmful for a specific category.
#[derive(Debug, Deserialize)]
pub(super) struct GoogleSafetyRating {
    /// The category of potential harm.
    ///
    /// Categories include:
    /// - HARM_CATEGORY_HARASSMENT
    /// - HARM_CATEGORY_HATE_SPEECH
    /// - HARM_CATEGORY_SEXUALLY_EXPLICIT
    /// - HARM_CATEGORY_DANGEROUS_CONTENT
    #[allow(dead_code)]
    category: String,

    /// The probability level of harm.
    ///
    /// Values include:
    /// - NEGLIGIBLE: Negligible probability of harm
    /// - LOW: Low probability of harm
    /// - MEDIUM: Medium probability of harm
    /// - HIGH: High probability of harm
    #[allow(dead_code)]
    probability: String,
}

/// Token usage statistics for a generation request.
///
/// Provides detailed token counts for billing and usage tracking.
#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub(super) struct GoogleUsageMetadata {
    /// Number of tokens in the prompt (input).
    pub(super) prompt_token_count: i32,

    /// Number of tokens in the generated response (output).
    pub(super) candidates_token_count: i32,

    /// Total number of tokens (prompt + candidates).
    pub(super) total_token_count: i32,
}

/// Response from listing available Gemini models.
///
/// Contains a list of models available for use with the Gemini API.
/// Documented in the [Google AI API Reference](https://ai.google.dev/api/models/list).
#[derive(Debug, Deserialize)]
pub(super) struct GoogleModelsResponse {
    /// List of available models.
    pub(super) models: Vec<GoogleModel>,

    /// Token for fetching the next page of results.
    ///
    /// If present, there are more models available that can be fetched
    /// by including this token in the next request.
    #[allow(dead_code)]
    pub(super) next_page_token: Option<String>,
}

/// Describes a Gemini model available for use.
///
/// Contains metadata about a specific model including its capabilities.
#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub(super) struct GoogleModel {
    /// The resource name of the model.
    ///
    /// Format: `models/{model}` where `{model}` is the model ID.
    /// Example: "models/gemini-1.5-pro"
    pub(super) name: String,

    /// List of generation methods supported by this model.
    ///
    /// Common values include:
    /// - "generateContent": Standard text generation
    /// - "streamGenerateContent": Streaming text generation
    /// - "countTokens": Token counting
    /// - "embedContent": Generate embeddings
    pub(super) supported_generation_methods: Vec<String>,
}

impl From<GoogleGenerateResponse> for ChatCompletionResponse {
    fn from(response: GoogleGenerateResponse) -> Self {
        let candidate = response
            .candidates
            .into_iter()
            .next()
            .expect("No candidates in Google response");

        let message_content = candidate
            .content
            .parts
            .iter()
            .filter_map(|part| part.text.as_ref())
            .cloned()
            .collect::<Vec<_>>()
            .join("");

        let finish_reason = candidate.finish_reason.as_ref().map_or_else(
            || {
                log::warn!("Google API response missing finish_reason, defaulting to 'stop'");
                crate::messages::FinishReason::Stop
            },
            |reason| match reason {
                FinishReason::Stop => crate::messages::FinishReason::Stop,
                FinishReason::MaxTokens => crate::messages::FinishReason::Length,
                FinishReason::Safety => crate::messages::FinishReason::ContentFilter,
                FinishReason::Recitation => crate::messages::FinishReason::ContentFilter,
                FinishReason::Other => crate::messages::FinishReason::Stop,
                FinishReason::Unknown(s) => {
                    log::warn!("Unknown finish reason from Google: {s}");
                    crate::messages::FinishReason::Other(s.clone())
                }
            },
        );

        Self {
            id: format!("gen-{}", uuid::Uuid::new_v4()),
            object: ObjectType::ChatCompletion,
            created: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap_or_default()
                .as_secs(),
            model: String::new(), // Will be set by the provider
            choices: vec![ChatChoice {
                index: candidate.index as u32,
                message: ChatMessage {
                    role: ChatRole::Assistant,
                    content: message_content,
                },
                finish_reason,
            }],
            usage: response.usage_metadata.map_or_else(
                || {
                    // If usage metadata is missing, provide default values
                    log::warn!("Google API response missing usage_metadata, using default values");
                    Usage {
                        prompt_tokens: 0,
                        completion_tokens: 0,
                        total_tokens: 0,
                    }
                },
                |metadata| Usage {
                    prompt_tokens: metadata.prompt_token_count as u32,
                    completion_tokens: metadata.candidates_token_count as u32,
                    total_tokens: metadata.total_token_count as u32,
                },
            ),
        }
    }
}

impl From<GoogleModel> for Model {
    fn from(model: GoogleModel) -> Self {
        // Extract model ID from the full name (e.g., "models/gemini-pro" -> "gemini-pro")
        let id = model.name.split('/').next_back().unwrap_or(&model.name).to_string();

        Self {
            id,
            object: ObjectType::Model,
            created: 0, // Google doesn't provide creation timestamps
            owned_by: "google".to_string(),
        }
    }
}

// Streaming types for Google Gemini SSE responses

/// Google streaming chunk format with borrowed strings for zero-copy parsing.
///
/// Google's streaming format uses Server-Sent Events with a structure similar
/// to their non-streaming API, but with incremental text delivered in parts.
/// Unlike OpenAI and Anthropic, Google maintains the candidates array structure
/// even in streaming mode.
///
/// See: https://ai.google.dev/gemini-api/docs/text-generation?lang=rest#stream-response-body
///
/// Each SSE event contains a complete JSON object with candidates array,
/// where each candidate contains incremental text in the parts array.
#[derive(Debug, Deserialize)]
pub(super) struct GoogleStreamChunk<'a> {
    /// Array of response candidates.
    ///
    /// Usually contains a single candidate (index 0) unless candidate_count > 1.
    /// Each candidate represents a different possible completion.
    pub candidates: Vec<GoogleStreamCandidate<'a>>,

    /// Model version information.
    ///
    /// Format: "gemini-1.5-flash-001" or similar
    /// Indicates the specific model version used for generation.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[allow(dead_code)]
    pub model_version: Option<Cow<'a, str>>,

    /// Token usage statistics.
    ///
    /// Only present in the final chunk when streaming is complete.
    /// Contains:
    /// - `promptTokenCount`: Number of tokens in the prompt
    /// - `candidatesTokenCount`: Number of tokens generated
    /// - `totalTokenCount`: Sum of prompt and candidates tokens
    #[serde(skip_serializing_if = "Option::is_none")]
    pub usage_metadata: Option<GoogleUsageMetadata>,
}

/// A streaming candidate from Google.
///
/// Represents one possible completion being generated.
/// In streaming mode, each chunk contains partial content that should
/// be concatenated to build the complete response.
#[derive(Debug, Deserialize)]
pub(super) struct GoogleStreamCandidate<'a> {
    /// Content with incremental text.
    ///
    /// Contains the role and parts array with text fragments.
    /// Each chunk adds more text to build the complete message.
    pub content: GoogleStreamContent<'a>,

    /// The reason generation stopped for this candidate.
    ///
    /// Only present in the final chunk for this candidate.
    /// Possible values:
    /// - "STOP": Natural stopping point reached
    /// - "MAX_TOKENS": Maximum token limit reached
    /// - "SAFETY": Response filtered for safety reasons
    /// - "RECITATION": Response filtered for recitation/plagiarism
    /// - "OTHER": Other stopping reason
    #[serde(skip_serializing_if = "Option::is_none")]
    pub finish_reason: Option<Cow<'a, str>>,

    /// Zero-based index of this candidate.
    ///
    /// Used to track multiple parallel completions when candidate_count > 1.
    /// Most requests have only one candidate (index: 0).
    #[allow(dead_code)]
    pub index: i32,

    /// Safety ratings for harmful content categories.
    ///
    /// Array of safety ratings for different harm categories:
    /// - HARM_CATEGORY_HARASSMENT
    /// - HARM_CATEGORY_HATE_SPEECH
    /// - HARM_CATEGORY_SEXUALLY_EXPLICIT
    /// - HARM_CATEGORY_DANGEROUS_CONTENT
    ///
    /// Each rating includes:
    /// - `category`: The harm category
    /// - `probability`: NEGLIGIBLE, LOW, MEDIUM, HIGH
    /// - `blocked`: Whether content was blocked for this category
    #[serde(skip_serializing_if = "Option::is_none")]
    #[allow(dead_code)]
    pub safety_ratings: Option<Vec<sonic_rs::Value>>,
}

/// Streaming content from Google.
///
/// Contains the actual text content being generated and metadata
/// about who is generating it (role).
#[derive(Debug, Deserialize)]
pub(super) struct GoogleStreamContent<'a> {
    /// Parts containing the incremental text.
    ///
    /// In streaming mode, typically contains a single part with a text fragment.
    /// Multiple parts might be present for multi-modal responses (text + code, etc.).
    /// Concatenate all text parts to build the complete response.
    pub parts: Vec<GoogleStreamPart<'a>>,

    /// Role of the content author.
    ///
    /// Always "model" for AI-generated responses.
    /// Other possible values: "user" (in conversation history)
    #[allow(dead_code)]
    pub role: Cow<'a, str>,
}

/// A streaming part with text content.
///
/// Represents a fragment of content in Google's streaming response.
/// Parts are the atomic units of content that get concatenated.
#[derive(Debug, Deserialize)]
pub(super) struct GoogleStreamPart<'a> {
    /// The incremental text content.
    ///
    /// Contains a text fragment that should be appended to previous parts.
    /// Can be as small as a single character or as large as multiple sentences.
    /// Empty strings may appear, especially at the start or end of streaming.
    pub text: Cow<'a, str>,
}

impl<'a> GoogleStreamChunk<'a> {
    /// Convert Google's streaming format to OpenAI-compatible ChatCompletionChunk.
    ///
    /// Maps Google's candidates/parts structure to OpenAI's simpler delta format.
    /// Handles finish reasons and usage statistics when present.
    pub(super) fn into_chunk(self, provider_name: &str, model_name: &str) -> ChatCompletionChunk {
        // Google sends one candidate at a time in streaming
        let candidate = self.candidates.first();

        let (content, finish_reason) = if let Some(candidate) = candidate {
            // Extract text from parts
            let text = candidate
                .content
                .parts
                .first()
                .map(|part| part.text.clone().into_owned());

            // Map finish reason
            let finish = candidate.finish_reason.as_ref().map(|reason| match reason.as_ref() {
                "STOP" => crate::messages::FinishReason::Stop,
                "MAX_TOKENS" => crate::messages::FinishReason::Length,
                "SAFETY" | "RECITATION" => crate::messages::FinishReason::ContentFilter,
                "OTHER" => crate::messages::FinishReason::Stop,
                other => crate::messages::FinishReason::Other(other.to_string()),
            });

            (text, finish)
        } else {
            (None, None)
        };

        // Generate a unique ID for this stream
        let id = format!("gen-{}", uuid::Uuid::new_v4());

        ChatCompletionChunk {
            id,
            object: ObjectType::ChatCompletionChunk,
            created: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap_or_default()
                .as_secs(),
            model: format!("{provider_name}/{model_name}"),
            choices: vec![ChatChoiceDelta {
                index: 0,
                delta: ChatMessageDelta {
                    role: if content.is_some() {
                        Some(ChatRole::Assistant)
                    } else {
                        None
                    },
                    content,
                    tool_calls: None,
                    function_call: None,
                },
                finish_reason,
                logprobs: None,
            }],
            system_fingerprint: None,
            usage: self.usage_metadata.map(|metadata| Usage {
                prompt_tokens: metadata.prompt_token_count as u32,
                completion_tokens: metadata.candidates_token_count as u32,
                total_tokens: metadata.total_token_count as u32,
            }),
        }
    }
}
