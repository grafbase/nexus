use std::fmt;

use serde::{Deserialize, Serialize};

/// OpenAI-compatible chat completion request.
///
/// This is the standard request format used across the LLM routing system.
/// It follows the OpenAI API specification, making it compatible with most
/// LLM providers and client libraries. The router translates this common
/// format to provider-specific formats as needed.
#[derive(Debug, Clone, Deserialize)]
pub(crate) struct ChatCompletionRequest {
    /// The conversation history as a list of messages.
    ///
    /// Messages should typically alternate between user and assistant roles,
    /// with an optional system message at the beginning to set context.
    pub(crate) messages: Vec<ChatMessage>,

    /// The model identifier in the format "provider/model".
    ///
    /// Examples:
    /// - "openai/gpt-4"
    /// - "anthropic/claude-3-opus-20240229"
    /// - "google/gemini-1.5-pro"
    pub(crate) model: String,

    /// Controls randomness in the response.
    ///
    /// Range: 0.0 to 2.0 (provider-dependent)
    /// - Lower values (e.g., 0.2) make output more focused and deterministic
    /// - Higher values (e.g., 0.8) make output more random and creative
    /// - Default varies by provider
    #[serde(default)]
    pub(crate) temperature: Option<f32>,

    /// Maximum number of tokens to generate in the response.
    ///
    /// This limit includes the completion tokens only, not the prompt.
    /// Different models have different maximum limits.
    #[serde(default)]
    pub(crate) max_tokens: Option<u32>,

    /// Nucleus sampling parameter (alternative to temperature).
    ///
    /// Range: 0.0 to 1.0
    /// - The model considers tokens with top_p cumulative probability
    /// - E.g., 0.1 means only tokens in the top 10% probability are considered
    /// - Generally, alter temperature OR top_p, not both
    #[serde(default)]
    pub(crate) top_p: Option<f32>,

    /// Penalizes repeated tokens based on their frequency in the generated text.
    ///
    /// Range: -2.0 to 2.0
    /// - Positive values decrease likelihood of repeating the same text
    /// - Helps reduce repetitive output
    #[serde(default)]
    pub(crate) frequency_penalty: Option<f32>,

    /// Penalizes tokens based on whether they appear in the text at all.
    ///
    /// Range: -2.0 to 2.0
    /// - Positive values encourage the model to talk about new topics
    /// - Helps increase diversity of output
    #[serde(default)]
    pub(crate) presence_penalty: Option<f32>,

    /// Sequences that will cause the model to stop generating.
    ///
    /// When the model generates any of these sequences, it stops immediately.
    /// Useful for controlling output format or length.
    #[serde(default)]
    pub(crate) stop: Option<Vec<String>>,

    /// Whether to stream the response token by token.
    ///
    /// When true, responses are sent as Server-Sent Events.
    /// Note: Not all providers support streaming.
    #[serde(default)]
    pub(crate) stream: Option<bool>,
}

/// Role of a message sender in a chat conversation.
///
/// Defines who is sending a message in the conversation. Different providers
/// may use slightly different role names, but this enum covers the standard
/// roles used across all major LLM APIs.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub(crate) enum ChatRole {
    /// User input message.
    ///
    /// Represents messages from the human user asking questions or giving instructions.
    User,

    /// System instruction message.
    ///
    /// Sets the context, personality, or behavior for the assistant.
    /// Usually placed at the beginning of the conversation.
    /// Not all providers support system messages in the same way.
    System,

    /// Assistant/model response message.
    ///
    /// Represents messages generated by the AI model in response to user input.
    /// Some providers call this "model" instead of "assistant".
    Assistant,

    /// Any other role not yet known.
    ///
    /// Captures unknown role values for forward compatibility.
    /// This ensures the system doesn't break when providers add new roles.
    #[serde(untagged)]
    Other(String),
}

impl AsRef<str> for ChatRole {
    fn as_ref(&self) -> &str {
        match self {
            ChatRole::User => "user",
            ChatRole::System => "system",
            ChatRole::Assistant => "assistant",
            ChatRole::Other(s) => s,
        }
    }
}

impl fmt::Display for ChatRole {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        self.as_ref().fmt(f)
    }
}

/// A single message in a chat conversation.
///
/// Represents one turn in the conversation between the user and the assistant.
/// This format is compatible with OpenAI's API and is used as the common
/// format across all providers.
#[derive(Debug, Clone, Deserialize, Serialize)]
pub(crate) struct ChatMessage {
    /// The role of the message sender.
    pub(crate) role: ChatRole,

    /// The text content of the message.
    ///
    /// For user messages: the question or instruction
    /// For assistant messages: the model's response
    /// For system messages: the context or behavior instructions
    pub(crate) content: String,
}

/// API object type identifier for OpenAI-compatible responses.
///
/// Used to identify the type of object being returned in API responses.
/// This helps clients parse responses correctly and ensures compatibility
/// with OpenAI's API specification.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub(crate) enum ObjectType {
    /// Chat completion response object.
    ///
    /// Returned when requesting a chat completion.
    #[serde(rename = "chat.completion")]
    ChatCompletion,

    /// List object for collections.
    ///
    /// Used when returning lists of items (e.g., available models).
    #[serde(rename = "list")]
    List,

    /// Individual model object.
    ///
    /// Represents metadata about a single AI model.
    #[serde(rename = "model")]
    Model,

    /// Any other object type.
    ///
    /// Captures unknown object types for forward compatibility.
    #[serde(untagged)]
    Other(String),
}

/// OpenAI-compatible chat completion response.
///
/// The standard response format returned by the LLM router for all chat completions.
/// This format is compatible with OpenAI's API, making it easy to use with existing
/// client libraries and tools. Provider-specific responses are translated to this format.
#[derive(Debug, Clone, Serialize)]
pub(crate) struct ChatCompletionResponse {
    /// Unique identifier for this completion.
    ///
    /// Format varies by provider but is always unique.
    pub(crate) id: String,

    /// The object type, always "chat.completion" for chat responses.
    pub(crate) object: ObjectType,

    /// Unix timestamp (seconds since epoch) when the completion was created.
    pub(crate) created: u64,

    /// The model that generated the completion.
    ///
    /// Includes the provider prefix (e.g., "openai/gpt-4").
    pub(crate) model: String,

    /// The completion choices generated by the model.
    ///
    /// Usually contains one choice, but can have multiple if n > 1 was requested.
    pub(crate) choices: Vec<ChatChoice>,

    /// Token usage statistics for this completion.
    ///
    /// Used for tracking costs and usage limits.
    pub(crate) usage: Usage,
}

/// Reason why the model stopped generating tokens.
///
/// Indicates why the model stopped producing output. This is important for
/// understanding whether the response is complete or was cut off. The enum
/// is designed to be forward-compatible with new stop reasons from providers.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub(crate) enum FinishReason {
    /// Natural stop point or stop sequence encountered.
    ///
    /// The model completed its response naturally or hit a stop sequence.
    #[serde(rename = "stop")]
    Stop,

    /// Maximum token limit reached.
    ///
    /// The response was cut off because it hit the max_tokens limit.
    #[serde(rename = "length")]
    Length,

    /// Content filtered for safety/policy reasons.
    ///
    /// The model's response was blocked by content filters.
    #[serde(rename = "content_filter")]
    ContentFilter,

    /// Model invoked a tool or function call.
    ///
    /// The model is requesting to use an external tool.
    #[serde(rename = "tool_calls")]
    ToolCalls,

    /// Any other finish reason.
    ///
    /// Captures unknown finish reasons for forward compatibility.
    /// The string value contains the original reason from the provider.
    #[serde(untagged)]
    Other(String),
}

/// A single completion choice generated by the model.
///
/// Represents one possible completion for the given input. When multiple
/// choices are requested (n > 1), each will have a different index.
/// Each choice contains the generated message and metadata about why
/// generation stopped.
#[derive(Debug, Clone, Serialize)]
pub(crate) struct ChatChoice {
    /// Zero-based index of this choice in the list of choices.
    ///
    /// Used when multiple completions are requested (n > 1).
    pub(crate) index: u32,

    /// The message generated by the model for this choice.
    pub(crate) message: ChatMessage,

    /// The reason why the model stopped generating tokens for this choice.
    pub(crate) finish_reason: FinishReason,
}

/// Token usage statistics for a completion request.
///
/// Tracks the number of tokens used in both the prompt and the completion.
/// This information is crucial for understanding costs, as most LLM providers
/// charge based on token usage. Different models may count tokens differently.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub(crate) struct Usage {
    /// Number of tokens in the prompt (input).
    ///
    /// Includes all messages sent to the model, including system messages.
    pub(crate) prompt_tokens: u32,

    /// Number of tokens in the completion (output).
    ///
    /// The tokens generated by the model in its response.
    pub(crate) completion_tokens: u32,

    /// Total tokens used (prompt + completion).
    ///
    /// The sum of prompt_tokens and completion_tokens.
    pub(crate) total_tokens: u32,
}

/// Information about an available AI model.
///
/// Represents metadata about a model that can be used for completions.
/// This information is returned when listing available models and helps
/// clients understand what models are available and their characteristics.
#[derive(Debug, Clone, Serialize)]
pub(crate) struct Model {
    /// The model identifier.
    ///
    /// Format: "provider/model-name"
    /// Examples: "openai/gpt-4", "anthropic/claude-3-opus-20240229"
    pub(crate) id: String,

    /// The object type, always "model" for model objects.
    pub(crate) object: ObjectType,

    /// Unix timestamp when the model was created.
    ///
    /// May be 0 for providers that don't provide this information.
    pub(crate) created: u64,

    /// The organization that owns the model.
    ///
    /// Examples: "openai", "anthropic", "google"
    pub(crate) owned_by: String,
}

/// Response containing a list of available models.
///
/// Returns all models available through the LLM router from all configured
/// providers. Each model ID includes the provider prefix to avoid naming
/// conflicts between providers.
#[derive(Debug, Clone, Serialize)]
pub(crate) struct ModelsResponse {
    /// The object type, always "list" for list responses.
    pub(crate) object: ObjectType,

    /// List of available models from all providers.
    ///
    /// Models are prefixed with their provider name (e.g., "openai/gpt-4").
    pub(crate) data: Vec<Model>,
}

/// Represents a chunk in a streaming chat completion response.
///
/// When streaming is enabled, the response is sent as a series of chunks
/// via Server-Sent Events (SSE). Each chunk contains a delta of the message
/// being generated. This format is compatible with OpenAI's streaming API.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub(crate) struct ChatCompletionChunk {
    /// Unique identifier for this completion stream.
    ///
    /// Remains consistent across all chunks in the same stream.
    pub(crate) id: String,

    /// The object type, always "chat.completion.chunk" for streaming responses.
    pub(crate) object: String,

    /// Unix timestamp (seconds since epoch) when the chunk was created.
    pub(crate) created: u64,

    /// The model that is generating the completion.
    ///
    /// Includes the provider prefix (e.g., "openai/gpt-4").
    pub(crate) model: String,

    /// The delta choices for this chunk.
    ///
    /// Usually contains one choice, but can have multiple if n > 1 was requested.
    pub(crate) choices: Vec<ChatChoiceDelta>,

    /// System fingerprint for the model configuration.
    ///
    /// Optional field that some providers include for reproducibility.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(crate) system_fingerprint: Option<String>,
}

/// A single choice delta in a streaming response chunk.
///
/// Represents an incremental update to a choice being generated.
/// The client should concatenate these deltas to build the complete message.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub(crate) struct ChatChoiceDelta {
    /// Zero-based index of this choice in the list of choices.
    pub(crate) index: u32,

    /// The incremental message content for this chunk.
    pub(crate) delta: ChatMessageDelta,

    /// The reason why the model stopped generating tokens.
    ///
    /// Only present in the final chunk of a stream for each choice.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(crate) finish_reason: Option<FinishReason>,

    /// Log probability information for this chunk.
    ///
    /// Only present if requested in the completion parameters.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(crate) logprobs: Option<serde_json::Value>,
}

/// Incremental message content in a streaming chunk.
///
/// Contains partial content that should be appended to the message
/// being constructed. The role is typically only present in the first
/// chunk of a stream.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub(crate) struct ChatMessageDelta {
    /// The role of the message sender.
    ///
    /// Usually only present in the first chunk to indicate the assistant role.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(crate) role: Option<ChatRole>,

    /// The incremental text content to append.
    ///
    /// Can be an empty string, especially in the first or last chunks.
    /// Clients should concatenate all non-None content values.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(crate) content: Option<String>,

    /// Tool calls being constructed incrementally.
    ///
    /// Used when the model is invoking functions or tools.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(crate) tool_calls: Option<Vec<serde_json::Value>>,

    /// Function call being constructed incrementally.
    ///
    /// Deprecated in favor of tool_calls but still supported for compatibility.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(crate) function_call: Option<serde_json::Value>,
}
